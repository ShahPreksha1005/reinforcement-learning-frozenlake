# -*- coding: utf-8 -*-
"""reinforcement-learning-frozenlake.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CKuLZOaCJuzvUsLpksftHL0cJQmLFy_K

# **Assignment: Familiarization with OpenAI Gym for Reinforcement Learning**
## **Topic: CartPole Environment**

---

### **Libraries and Dependencies**

#### **Importing necessary libraries:**
 - gym: For creating and managing RL environments.
 - numpy: For numerical operations (not heavily used in this basic version).
 - matplotlib.pyplot: For visualizing the environment states.
 - IPython.display.clear_output: To clear previous output in Jupyter for smooth visualization.
"""

import gym
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import clear_output

"""#### **1. Environment Creation**

Create the CartPole environment using the gym library.

*CartPole-v1* is a classic control problem where the agent must balance a pole on a cart.
The environment consists of states, actions, and rewards.
   
#### **Key Concepts:**
 - **State**: The current position and velocity of the cart and pole.
 - **Action**: Either move left (0) or right (1).
 - **Reward**: A positive reward is given for every timestep the pole remains upright.

"""

# Create the CartPole environment
env = gym.make('CartPole-v1')

"""### **2. Visualization Function**

#### Function Name: *visualize_cartpole*

Visualize the CartPole environment over a specified number of episodes.

#### **Parameters:**
- env: The CartPole environment instance.
- num_episodes: The number of episodes to run and visualize (default is 5).

#### **2.1. Episode Initialization**
        
- Reset the environment to start a new episode.
- The done flag will indicate when the episode has ended (e.g., when the pole falls).
- *total_reward* keeps track of the accumulated reward for the episode.

#### **2.2. Episode Loop (Actions and Environment Updates)**
Continue the episode until it's done.

#### **2.3. Environment Rendering**
Render the environment to get the current state image in 'rgb_array' mode.

#### **2.4. Random Action Selection**
- In this example, the agent selects a random action (either move left or right).
- In actual RL, the agent would use a policy to decide the best action.
          
#### 2.5. **Step Function (Environment Transition)**
- The *env.step(action)* function performs the action and returns:
- *next_state*: The new state after the action.
- *reward*: Reward earned for this action.
- *done*: Whether the episode has ended (True if the pole has fallen).

"""

def visualize_cartpole(env, num_episodes=5):

    # List to store total rewards for each episode.
    total_rewards = []

    # Loop through each episode
    for episode in range(num_episodes):

        # 2.1. Episode Initialization
        state = env.reset()
        done = False
        total_reward = 0  # Total rewards accumulated in the episode

        print(f"Episode {episode + 1}:")  # Display the episode number

        # 2.2. Episode Loop (Actions and Environment Updates)
        while not done:
            # Clear previous output to make visualization smoother.
            clear_output(wait=True)

            # 2.3. Environment Rendering
            img = env.render(mode='rgb_array')

            # Display the current state of the CartPole environment.
            plt.imshow(img)
            plt.axis('off')  # Turn off the axis for better visuals
            plt.title(f'Episode {episode + 1} | Total Reward: {total_reward}')  # Display the current episode and reward
            plt.show()

            # 2.4. Random Action Selection
            action = env.action_space.sample()

            # 2.5. Step Function (Environment Transition)
            next_state, reward, done, info = env.step(action)

            # Update the total reward for this episode.
            total_reward += reward

            # Set the current state to the next state for the next iteration.
            state = next_state

        # Store the total reward for the completed episode.
        total_rewards.append(total_reward)
        print(f"Total Reward: {total_reward}\n")  # Display total reward for the episode

    # 3. Environment Closure
    env.close()

"""#### **4. Running the Visualization**
Execute the *visualize_cartpole* function to simulate and visualize the CartPole environment.

This runs the simulation for 5 episodes by default.

"""

# 4. Running the Visualization
visualize_cartpole(env, num_episodes=5)

import gym
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import clear_output

# Create the CartPole environment
env = gym.make('CartPole-v1')

# Function to visualize the environment and plot rewards
def visualize_cartpole_with_rewards(env, num_episodes=10):
    total_rewards = []  # List to store total rewards for each episode

    for episode in range(num_episodes):
        state = env.reset()  # Reset the environment to start a new episode
        done = False  # Flag to check if the episode is done
        total_reward = 0  # Variable to accumulate rewards for the episode

        print(f"Episode {episode + 1}:")

        while not done:
            clear_output(wait=True)  # Clear previous output in the notebook

            # Render the environment in real-time (can be disabled for faster runs)
            env.render()

            # Choose a random action (either move left or right)
            action = env.action_space.sample()

            # Take the action and observe the next state and reward
            next_state, reward, done, info = env.step(action)

            # Update the total reward
            total_reward += reward

            # Update state for the next iteration
            state = next_state

        total_rewards.append(total_reward)  # Store the total reward for this episode
        print(f"Total Reward: {total_reward}\n")

    env.close()  # Close the environment when done

    # Plotting the rewards
    plt.figure(figsize=(10, 5))
    plt.plot(total_rewards, marker='o', linestyle='-', color='b', label="Total Reward per Episode")
    plt.title('Rewards per Episode in CartPole')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.grid(True)
    plt.legend()
    plt.show()

# Run the interaction with the CartPole environment
visualize_cartpole_with_rewards(env, num_episodes=10)

"""The graph shows the total reward earned per episode in the CartPole environment over 10 episodes.

Key inferences include:

- Performance is inconsistent, with rewards varying significantly across episodes.
- There are peaks in episodes 3 and 7, indicating better stability in balancing the pole during these episodes.
- Sharp drops in reward at the beginning and end suggest episodes where the agent failed early, demonstrating the challenge of balancing the pole.

This randomness is likely due to the use of random actions rather than a trained policy, showing the need for reinforcement learning algorithms to improve performance.

---

### **Key Learnings and Insights**

1. **Introduction to Reinforcement Learning**:
   - Learned the basics of RL, including state, action, and reward interactions using the CartPole environment.
   - Random actions highlighted the importance of learning policies for better performance.

2. **Gym Library and Environment Setup**:
   - Gained hands-on experience with OpenAI Gym for creating RL environments.
   - Explored state and action spaces and how they impact the environment dynamics.

3. **Rewards and Performance Visualization**:
   - Implemented reward tracking and plotted total rewards per episode, which provided insights into the agent's performance over time.

---

### **Conclusion**

The assignment introduced OpenAI Gym and CartPole, showcasing how agents interact with environments using random actions. While random actions led to inconsistent results, this forms a foundation for exploring more advanced RL algorithms to optimize agent performance in future tasks.

---
---
"""

